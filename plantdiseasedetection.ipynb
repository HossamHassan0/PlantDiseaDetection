{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-11T13:15:28.147366Z","iopub.execute_input":"2022-03-11T13:15:28.147941Z","iopub.status.idle":"2022-03-11T13:15:43.474763Z","shell.execute_reply.started":"2022-03-11T13:15:28.147852Z","shell.execute_reply":"2022-03-11T13:15:43.474053Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Import Libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport glob\nimport matplotlib.pyplot as plt\n# Keras API\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Flatten\nfrom keras.layers import Conv2D,MaxPooling2D,Activation,AveragePooling2D,BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:16:32.825958Z","iopub.execute_input":"2022-03-11T13:16:32.826202Z","iopub.status.idle":"2022-03-11T13:16:37.559619Z","shell.execute_reply.started":"2022-03-11T13:16:32.826173Z","shell.execute_reply":"2022-03-11T13:16:37.556484Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# My data is in google drive.\ntrain_dir =\"../input/plants/train\"\ntest_dir=\"../input/test-data/test\"","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:17:16.046017Z","iopub.execute_input":"2022-03-11T13:17:16.046647Z","iopub.status.idle":"2022-03-11T13:17:16.051420Z","shell.execute_reply.started":"2022-03-11T13:17:16.046608Z","shell.execute_reply":"2022-03-11T13:17:16.050587Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# function to get count of images\ndef get_files(directory):\n  if not os.path.exists(directory):\n    return 0\n  count=0\n  for current_path,dirs,files in os.walk(directory):\n    for dr in dirs:\n      count+= len(glob.glob(os.path.join(current_path,dr+\"/*\")))\n  return count       ","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:18:00.373474Z","iopub.execute_input":"2022-03-11T13:18:00.373731Z","iopub.status.idle":"2022-03-11T13:18:00.379385Z","shell.execute_reply.started":"2022-03-11T13:18:00.373701Z","shell.execute_reply":"2022-03-11T13:18:00.378647Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_samples =get_files(train_dir)\nnum_classes=len(glob.glob(train_dir+\"/*\"))\ntest_samples=get_files(test_dir) # For testing i took only few samples from unseen data. we can evaluate using validation data which is part of train data.\nprint(num_classes,\"Classes\")\nprint(train_samples,\"Train images\")\nprint(test_samples,\"Test images\")","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:18:12.889690Z","iopub.execute_input":"2022-03-11T13:18:12.890053Z","iopub.status.idle":"2022-03-11T13:18:17.916239Z","shell.execute_reply.started":"2022-03-11T13:18:12.890015Z","shell.execute_reply":"2022-03-11T13:18:17.915381Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Preprocessing data.\ntrain_datagen=ImageDataGenerator(rescale=1./255,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   validation_split=0.2, # validation split 20%.\n                                   horizontal_flip=True)\ntest_datagen=ImageDataGenerator(rescale=1./255)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:18:31.791058Z","iopub.execute_input":"2022-03-11T13:18:31.791312Z","iopub.status.idle":"2022-03-11T13:18:31.796245Z","shell.execute_reply.started":"2022-03-11T13:18:31.791283Z","shell.execute_reply":"2022-03-11T13:18:31.795393Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# set height and width and color of input image.\nimg_width,img_height =256,256\ninput_shape=(img_width,img_height,3)\nbatch_size =32\n\ntrain_generator =train_datagen.flow_from_directory(train_dir,\n                                                   target_size=(img_width,img_height),\n                                                   batch_size=batch_size)\ntest_generator=test_datagen.flow_from_directory(test_dir,shuffle=True,\n                                                   target_size=(img_width,img_height),\n                                                   batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:18:45.119794Z","iopub.execute_input":"2022-03-11T13:18:45.120062Z","iopub.status.idle":"2022-03-11T13:18:51.380077Z","shell.execute_reply.started":"2022-03-11T13:18:45.120032Z","shell.execute_reply":"2022-03-11T13:18:51.379369Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# The name of the 12 diseases.\ntrain_generator.class_indices","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:19:03.795926Z","iopub.execute_input":"2022-03-11T13:19:03.796187Z","iopub.status.idle":"2022-03-11T13:19:03.803762Z","shell.execute_reply.started":"2022-03-11T13:19:03.796159Z","shell.execute_reply":"2022-03-11T13:19:03.803092Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# CNN building.\nmodel = Sequential()\nmodel.add(Conv2D(32, (5, 5),input_shape=input_shape,activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Conv2D(32, (3, 3),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(64, (3, 3),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))   \nmodel.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128,activation='relu'))          \nmodel.add(Dense(num_classes,activation='softmax'))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:19:18.977391Z","iopub.execute_input":"2022-03-11T13:19:18.978134Z","iopub.status.idle":"2022-03-11T13:19:21.414740Z","shell.execute_reply.started":"2022-03-11T13:19:18.978097Z","shell.execute_reply":"2022-03-11T13:19:21.413951Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model_layers = [ layer.name for layer in model.layers]\nprint('layer name : ',model_layers)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:19:35.367941Z","iopub.execute_input":"2022-03-11T13:19:35.368193Z","iopub.status.idle":"2022-03-11T13:19:35.373958Z","shell.execute_reply.started":"2022-03-11T13:19:35.368164Z","shell.execute_reply":"2022-03-11T13:19:35.373240Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Take one image to visualize it's changes after every layer\nfrom keras.preprocessing import image\nimport numpy as np\nimg1 = image.load_img('../input/plants/train/Strawberry___Leaf_scorch/0024203d-6e4c-490f-b9a8-e5926df0b76e___RS_L.Scorch 0795.JPG')\nplt.imshow(img1);\n\n#preprocess image\nimg1 = image.load_img('../input/plants/train/Strawberry___Leaf_scorch/0024203d-6e4c-490f-b9a8-e5926df0b76e___RS_L.Scorch 0795.JPG', target_size=(256, 256))\nimg = image.img_to_array(img1)\nimg = img/255\nimg = np.expand_dims(img, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:21:06.093094Z","iopub.execute_input":"2022-03-11T13:21:06.093668Z","iopub.status.idle":"2022-03-11T13:21:06.310784Z","shell.execute_reply.started":"2022-03-11T13:21:06.093628Z","shell.execute_reply":"2022-03-11T13:21:06.310169Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Visualizing output after every layer.\nfrom keras.models import Model\nconv2d_1_output = Model(inputs=model.input, outputs=model.get_layer('conv2d').output)\nmax_pooling2d_1_output = Model(inputs=model.input,outputs=model.get_layer('max_pooling2d').output)\nconv2d_2_output = Model(inputs=model.input,outputs=model.get_layer('conv2d_1').output)\nmax_pooling2d_2_output = Model(inputs=model.input,outputs=model.get_layer('max_pooling2d_1').output)\nconv2d_3_output = Model(inputs=model.input,outputs=model.get_layer('conv2d_2').output)\nmax_pooling2d_3_output = Model(inputs=model.input,outputs=model.get_layer('max_pooling2d_2').output)\nflatten_1_output = Model(inputs=model.input,outputs=model.get_layer('flatten').output)\nconv2d_1_features = conv2d_1_output.predict(img)\nmax_pooling2d_1_features = max_pooling2d_1_output.predict(img)\nconv2d_2_features = conv2d_2_output.predict(img)\nmax_pooling2d_2_features = max_pooling2d_2_output.predict(img)\nconv2d_3_features = conv2d_3_output.predict(img)\nmax_pooling2d_3_features = max_pooling2d_3_output.predict(img)\nflatten_1_features = flatten_1_output.predict(img)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:21:38.590792Z","iopub.execute_input":"2022-03-11T13:21:38.591055Z","iopub.status.idle":"2022-03-11T13:21:49.449852Z","shell.execute_reply.started":"2022-03-11T13:21:38.591024Z","shell.execute_reply":"2022-03-11T13:21:49.449097Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import matplotlib.image as mpimg\n\nfig=plt.figure(figsize=(14,7))\ncolumns = 8\nrows = 4\nfor i in range(columns*rows):\n    #img = mpimg.imread()\n    fig.add_subplot(rows, columns, i+1)\n    plt.axis('off')\n    plt.title('filter'+str(i))\n    plt.imshow(conv2d_1_features[0, :, :, i], cmap='viridis') # Visualizing in color mode.\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:21:53.267991Z","iopub.execute_input":"2022-03-11T13:21:53.268248Z","iopub.status.idle":"2022-03-11T13:21:54.683811Z","shell.execute_reply.started":"2022-03-11T13:21:53.268219Z","shell.execute_reply":"2022-03-11T13:21:54.683194Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import matplotlib.image as mpimg\n\nfig=plt.figure(figsize=(14,7))\ncolumns = 8\nrows = 4\nfor i in range(columns*rows):\n    #img = mpimg.imread()\n    fig.add_subplot(rows, columns, i+1)\n    plt.axis('off')\n    plt.title('filter'+str(i))\n    plt.imshow(max_pooling2d_1_features[0, :, :, i], cmap='viridis')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:22:05.549813Z","iopub.execute_input":"2022-03-11T13:22:05.550077Z","iopub.status.idle":"2022-03-11T13:22:06.661911Z","shell.execute_reply.started":"2022-03-11T13:22:05.550048Z","shell.execute_reply":"2022-03-11T13:22:06.661244Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import matplotlib.image as mpimg\n\nfig=plt.figure(figsize=(14,7))\ncolumns = 8\nrows = 4\nfor i in range(columns*rows):\n    #img = mpimg.imread()\n    fig.add_subplot(rows, columns, i+1)\n    plt.axis('off')\n    plt.title('filter'+str(i))\n    plt.imshow(conv2d_2_features[0, :, :, i], cmap='viridis')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:22:17.734976Z","iopub.execute_input":"2022-03-11T13:22:17.735228Z","iopub.status.idle":"2022-03-11T13:22:19.079697Z","shell.execute_reply.started":"2022-03-11T13:22:17.735199Z","shell.execute_reply":"2022-03-11T13:22:19.078215Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# we can also visualize in color mode.\nimport matplotlib.image as mpimg\n\nfig=plt.figure(figsize=(14,7))\ncolumns = 8\nrows = 4\nfor i in range(columns*rows):\n    #img = mpimg.imread()\n    fig.add_subplot(rows, columns, i+1)\n    plt.axis('off')\n    plt.title('filter'+str(i))\n    plt.imshow(max_pooling2d_2_features[0, :, :, i], cmap='viridis') \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:22:27.150868Z","iopub.execute_input":"2022-03-11T13:22:27.151129Z","iopub.status.idle":"2022-03-11T13:22:28.865878Z","shell.execute_reply.started":"2022-03-11T13:22:27.151101Z","shell.execute_reply":"2022-03-11T13:22:28.865265Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import matplotlib.image as mpimg\n\nfig=plt.figure(figsize=(16,16))\ncolumns =8 \nrows = 8\nfor i in range(columns*rows):\n    #img = mpimg.imread()\n    fig.add_subplot(rows, columns, i+1)\n    plt.axis('off')\n    plt.title('filter'+str(i))\n    plt.imshow(conv2d_3_features[0, :, :, i], cmap='viridis')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:22:38.815866Z","iopub.execute_input":"2022-03-11T13:22:38.816137Z","iopub.status.idle":"2022-03-11T13:22:41.244803Z","shell.execute_reply.started":"2022-03-11T13:22:38.816106Z","shell.execute_reply":"2022-03-11T13:22:41.244193Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import matplotlib.image as mpimg\n\nfig=plt.figure(figsize=(14,14))\ncolumns = 8\nrows = 8\nfor i in range(columns*rows):\n    #img = mpimg.imread()\n    fig.add_subplot(rows, columns, i+1)\n    plt.axis('off')\n    plt.title('filter'+str(i))\n    plt.imshow(max_pooling2d_3_features[0, :, :, i],cmap='viridis')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:22:49.206130Z","iopub.execute_input":"2022-03-11T13:22:49.206404Z","iopub.status.idle":"2022-03-11T13:22:51.639912Z","shell.execute_reply.started":"2022-03-11T13:22:49.206373Z","shell.execute_reply":"2022-03-11T13:22:51.639176Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# validation data.\nvalidation_generator = train_datagen.flow_from_directory(\n    train_dir, # same directory as training data\n    target_size=(img_height, img_width),\n    batch_size=batch_size) ","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:23:08.349249Z","iopub.execute_input":"2022-03-11T13:23:08.349800Z","iopub.status.idle":"2022-03-11T13:23:13.904177Z","shell.execute_reply.started":"2022-03-11T13:23:08.349760Z","shell.execute_reply":"2022-03-11T13:23:13.902620Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Model building to get trained with parameters.\nimport tensorflow as tf\n\nopt = tf.keras.optimizers.Adam(lr=0.001)\nmodel.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\ntrain=model.fit_generator(train_generator,\n                          epochs=15,\n                          steps_per_epoch=train_generator.samples // batch_size,\n                          validation_data=validation_generator,\n                          verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:24:48.339393Z","iopub.execute_input":"2022-03-11T13:24:48.339651Z","iopub.status.idle":"2022-03-11T16:44:51.002951Z","shell.execute_reply.started":"2022-03-11T13:24:48.339621Z","shell.execute_reply":"2022-03-11T16:44:51.002145Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"acc = train.history['accuracy']\nval_acc = train.history['val_accuracy']\nloss = train.history['loss']\nval_loss = train.history['val_loss']\nepochs = range(1, len(acc) + 1)\n#Train and validation accuracy\nplt.plot(epochs, acc, 'b', label='Training accurarcy')\nplt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\nplt.title('Training and Validation accurarcy')\nplt.legend()\n\nplt.figure()\n#Train and validation loss\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:45:21.927626Z","iopub.execute_input":"2022-03-11T16:45:21.927888Z","iopub.status.idle":"2022-03-11T16:45:22.311306Z","shell.execute_reply.started":"2022-03-11T16:45:21.927853Z","shell.execute_reply":"2022-03-11T16:45:22.310648Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Save entire model with optimizer, architecture, weights and training configuration.\nfrom keras.models import load_model\nmodel.save('savedmodel.h5')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:46:22.306675Z","iopub.execute_input":"2022-03-11T16:46:22.306940Z","iopub.status.idle":"2022-03-11T16:46:22.609633Z","shell.execute_reply.started":"2022-03-11T16:46:22.306911Z","shell.execute_reply":"2022-03-11T16:46:22.608817Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Save model weights.\nfrom keras.models import load_model\nmodel.save_weights('savedmodel_weights.h5')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:46:52.146832Z","iopub.execute_input":"2022-03-11T16:46:52.147103Z","iopub.status.idle":"2022-03-11T16:46:52.253282Z","shell.execute_reply.started":"2022-03-11T16:46:52.147074Z","shell.execute_reply":"2022-03-11T16:46:52.252559Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"classes = train_generator.class_indices \nclasses","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:47:10.632193Z","iopub.execute_input":"2022-03-11T16:47:10.632882Z","iopub.status.idle":"2022-03-11T16:47:10.639059Z","shell.execute_reply.started":"2022-03-11T16:47:10.632844Z","shell.execute_reply":"2022-03-11T16:47:10.638175Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Loading model and predict.\nfrom keras.models import load_model\nmodel=load_model('savedmodel.h5')\n\nClasses = [\"Apple___Cedar_apple_rust\",\"Apple___healthy\",\"Corn_(maize)___Common_rust_\",\"Corn_(maize)___healthy\",\"Pepper,_bell___Bacterial_spot\",\"Pepper,_bell___healthy\",\"Potato___Early_blight\",\"Potato___Late_blight\",\"Potato___healthy\",\"Strawberry___Leaf_scorch\",\"Strawberry___healthy\",\"Tomato___Early_blight\",\"Tomato___Late_blight\",\"Tomato___healthy\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:50:33.392970Z","iopub.execute_input":"2022-03-11T16:50:33.393229Z","iopub.status.idle":"2022-03-11T16:50:33.804917Z","shell.execute_reply.started":"2022-03-11T16:50:33.393199Z","shell.execute_reply":"2022-03-11T16:50:33.804150Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Pre-Processing test data same as train data.\nimg_width=256\nimg_height=256\n#model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\nfrom keras.preprocessing import image\n\ndef prepare(img_path):\n    img = image.load_img(img_path, target_size=(256, 256))\n    x = image.img_to_array(img)\n    x = x/255\n    return np.expand_dims(x, axis=0)\n    \n    \nresult = model.predict([prepare('../input/test-data/test/0a08af15-adfe-447c-8ed4-17ed2702d810___RS_L.Scorch 0054_flipLR.JPG')])\ndisease=image.load_img('../input/test-data/test/0a08af15-adfe-447c-8ed4-17ed2702d810___RS_L.Scorch 0054_flipLR.JPG')\nplt.imshow(disease)\nprint(Classes[int(result.argmax())])","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:55:10.383035Z","iopub.execute_input":"2022-03-11T16:55:10.383301Z","iopub.status.idle":"2022-03-11T16:55:10.661072Z","shell.execute_reply.started":"2022-03-11T16:55:10.383271Z","shell.execute_reply":"2022-03-11T16:55:10.660454Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\nopen (\"output.tflite\" , \"wb\") .write(tflite_model)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:57:44.742701Z","iopub.execute_input":"2022-03-11T16:57:44.743056Z","iopub.status.idle":"2022-03-11T16:57:49.031793Z","shell.execute_reply.started":"2022-03-11T16:57:44.743009Z","shell.execute_reply":"2022-03-11T16:57:49.030895Z"},"trusted":true},"execution_count":39,"outputs":[]}]}